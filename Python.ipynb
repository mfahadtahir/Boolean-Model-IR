{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 IR | Boolean Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStopWords():\n",
    "    f = open(\"stopwords.txt\", \"r\")\n",
    "    stopWords = f.read().split()\n",
    "  \n",
    "    # making a Frozen Set as we want to use the \"Hashing\" part of it when we are processing files to ignore stopWords\n",
    "    stepWordsStem = set()\n",
    "    # We are stemming Stop words too to be able to remove all stems of a stopWord \n",
    "    # i.e if have is a stopword we want to ignore have, having, haven't and so on all stems of have \n",
    "    \n",
    "    for stopword in stopWords:\n",
    "        stepWordsStem.add(ps.stem(stopword))\n",
    "    stepWordsStem = frozenset(stepWordsStem)\n",
    "    f.close()\n",
    "    print(stopWords)\n",
    "    return stepWordsStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding Words from Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'is', 'the', 'of', 'all', 'and', 'to', 'can', 'be', 'as', 'once', 'for', 'at', 'am', 'are', 'has', 'have', 'had', 'up', 'his', 'her', 'in', 'on', 'no', 'we', 'do']\n",
      "Dictionary Length:  6763\n"
     ]
    }
   ],
   "source": [
    "def readFile(file):\n",
    "#     f = open(f\"./ShortStories/{file}.txt\", encoding=\"utf8\")\n",
    "    f=open('./ShortStories/{}.txt'.format(file), encoding='utf-8')\n",
    "    \n",
    "    #   Read and Lower Case \n",
    "    data = f.read().lower()    \n",
    "        \n",
    "    #   Removing Punctuations\n",
    "    data = data.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # We have identified all unique characters and numbers and removed all others but alphabets       \n",
    "    # '9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n\n",
    "    data = data.translate(str.maketrans('','','1234567890©§“”‘’\"´¨¯—ªã'))\n",
    "        \n",
    "\n",
    "    # Word to word checking of file and adding to Dictionary\n",
    "    \n",
    "    for word in data.split():\n",
    "        \n",
    "        word = ps.stem(word)\n",
    "        if word not in stopWords:\n",
    "#             print(word, ps.stem(word))\n",
    "\n",
    "            if word in dictionary.keys():\n",
    "                dictionary[word].add(file)\n",
    "            else:\n",
    "                dictionary[word] = {file}\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "dictionary = {}\n",
    "ps = PorterStemmer()\n",
    "stopWords = readStopWords()\n",
    "# Run for Each File\n",
    "for x in range(1,51):\n",
    "    readFile(x)\n",
    "    \n",
    "    \n",
    "print(\"Dictionary Length: \", len(dictionary))\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dictionary.txt\", \"w\")\n",
    "\n",
    "with open('dictionary.txt', 'w') as the_file:\n",
    "    for word in dictionary:\n",
    "        f.write('{}\\n'.format(word))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"invertedIndex.txt\", \"w\")\n",
    "\n",
    "with open('invertedIndex.txt', 'w') as the_file:\n",
    "    for word in dictionary:\n",
    "        f.write('{}: {}\\n'.format(word, dictionary[word]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out': {49: {610, 313, 1393, 921, 347, 861}}, 'nothing': {49: {465, 1067}}, 'black': {50: {120}}, 'fingers': {49: {475}}, 'offence': {49: {1408, 1357}}, 'read': {49: {483}}, 'gateyou': {49: {350}}, 'our': {50: {54}}, 'land': {49: {307, 3, 357}}, 'potful': {49: {185}}, 'come': {50: {2, 79, 23}}, 'really': {49: {1363}}, 'kamyshevs': {49: {64, 105, 1306, 47}}, 'thats': {50: {75}}, 'police': {49: {1185, 309}}, 'board': {49: {125}}, 'beg': {50: {6}}, 'whining': {49: {1211}}, 'flashing': {49: {796}}, 'chatter': {49: {961}}, 'insult': {49: {1425, 887}}, 'kamyshev': {49: {1280, 1057, 130, 1442, 9, 942, 1074, 989, 1150}}, 'old': {50: {0, 7}}, 'esteemed': {49: {739}}, 'allows': {49: {407}}, 'pig': {49: {762, 766}}, 'properly': {49: {96, 506}}, 'bonne': {49: {78}}, 'make': {49: {859, 1389, 590}}, 'nearer': {49: {1188}}, 'russian': {49: {193, 261, 230, 269, 502, 541}}, 'coat': {49: {1396}}, 'dim': {50: {116}}, 'see': {49: {352, 1154, 1157}}, 'principle': {49: {747, 733}}, 'want': {49: {1070}}, 'gone': {49: {1133}}, 'former': {49: {87}}, 'eyes': {50: {107}}, 'elegance': {49: {1036}}, 'curls': {49: {1404}}, 'follow': {49: {1366}}, 'eats': {49: {131}}, 'become': {49: {74, 70}}, 'are': {50: {10}}, 'perhaps': {49: {517, 526}}, 'bless': {49: {779}}, 'sunday': {49: {4}}, 'monsieur': {49: {25, 786, 365, 862}}, 'baked': {49: {241}}, 'will': {50: {56, 64, 28, 60}}, 'fanatical': {49: {478}}, 'accursed': {49: {1258}}, 'goes': {50: {87}}, 'studies': {49: {503}}, 'fills': {49: {707}}, 'devoted': {49: {1312}}, 'impartial': {49: {760}}, 'joking': {49: {1192, 1278, 1198}}, 'wants': {49: {959}}, 'worst': {49: {878}}, 'lieutenants': {49: {71}}, 'anything': {49: {209}}, 'somewhere': {49: {1114, 1187, 484}}, 'ready': {49: {1419}}, 'brag': {49: {630}}, 'my': {49: {896, 291, 165, 1256, 169, 877, 1299, 1268, 1395, 1271, 1372}}, 'after': {49: {1058, 948, 1453, 151}}, 'simple': {50: {17}}, 'heart': {49: {1340}}, 'caf': {49: {703}}, 'mustard': {49: {201, 173, 159}}, 'restrain': {49: {730}}, 'him': {49: {1376, 516, 38, 1400, 442, 447}}, 'ties': {49: {1020}}, 'dear': {49: {1300}}, 'because': {49: {227}}, 'good': {49: {53}}, 'tell': {49: {298, 244}}, 'truth': {49: {536, 300}}, 'russians': {49: {496}}, 'despair': {49: {1251}}, 'pull': {49: {615}}, 'ah': {49: {1341}}, 'floor': {49: {1001, 434}}, 'afterwards': {49: {62}}, 'suppose': {49: {829}}, 'big': {50: {119}}, 'manners': {49: {400, 54}}, 'great': {49: {850, 1039}}, 'begins': {50: {113}}, 'speaking': {49: {539, 647}}, 'eating': {49: {17}}, 'dropping': {49: {1042}}, 'word': {49: {1298, 1333}}, 'indeed': {49: {777}}, 'beetles': {49: {218}}, 'an': {49: {544, 1033, 651, 591, 368, 127}}, 'devil': {49: {857}}, 'thinking': {49: {281, 378, 259}}, 'right': {50: {77}}, 'take': {50: {61}}, 'asked': {49: {320}}, 'immoral': {49: {652}}, 'wood': {49: {612}}, 'noodle': {49: {984}}, 'lips': {49: {256}}, 'eat': {49: {208, 251, 109, 424}}, 'sound': {50: {95}}, 'champoun': {49: {864, 194, 996, 519, 40, 72, 1178, 1228, 1434, 723, 1173, 1142, 1303, 26}}, 'there': {49: {680, 282, 379, 314, 324}}, 'instance': {49: {226, 667}}, 'doing': {49: {971, 1245}}, 'assents': {49: {195}}, 'second': {49: {1455}}, 'motive': {49: {1194}}, 'napoleons': {49: {848}}, 'use': {49: {531}}, 'shrieks': {49: {1302}}, 'stir': {49: {1218}}, 'apparently': {49: {114}}, 'ill': {50: {69}}, 'head': {50: {44}}, 'tenant': {49: {1373}}, 'pigs': {49: {1391}}, 'babble': {49: {107, 957}}, 'crayfish': {49: {425}}, 'four': {49: {1136}}, 'ham': {49: {224, 155}}, 'so': {49: {1463}}, 'except': {49: {202}}, 'going': {49: {1095}}, 'sharing': {49: {34}}, 'himself': {49: {408, 731}}, 'ashamed': {49: {752}}, 'alone': {50: {58, 68}}, 'put': {49: {1421}}, 'neat': {49: {29}}, 'rats': {49: {215}}, 'dies': {50: {102}}, 'nonsensical': {49: {587}}, 'and': {50: {99, 108, 117, 86}}, 'face': {49: {1160, 1438}}, 'dozes': {50: {110}}, 'flourishes': {49: {473}}, 'wiping': {49: {141}}, 'escort': {49: {1186}}, 'of': {50: {96}}, 'husband': {49: {695}}, 'clever': {49: {388}}, 'compact': {49: {1207}}, 'tray': {49: {277}}, 'deliberately': {49: {16}}, 'spluttering': {49: {867}}, 'dining': {49: {14}}, 'has': {49: {162, 964}}, 'prayerbooks': {49: {1018}}, 'syoma': {50: {104, 52, 63}}, 'letters': {49: {1215}}, 'all': {50: {76}}, 'servants': {49: {937}}, 'pounce': {49: {1139}}, 'alphonse': {49: {969, 1177, 1172}}, 'just': {49: {893}}, 'end': {49: {354, 1468}}, 'was': {50: {47}}, 'chair': {49: {417}}, 'whaat': {49: {825}}, 'other': {49: {601}}, 'books': {49: {493}}, 'understand': {49: {1291, 789}}, 'thickly': {49: {156}}, 'wonder': {49: {1102}}, 'as': {49: {520, 637, 1328, 1324, 133}}, 'no': {49: {353, 197, 965, 682, 524, 530, 562, 1467, 284, 381}}, 'god': {49: {1257, 778}}, 'clean': {49: {28}}, 'frowning': {49: {533}}, 'for': {50: {33}}, 'trunk': {49: {1026, 1050, 980, 1013}}, 'referring': {49: {643}}, 'by': {49: {1225, 619, 1305, 1401}}, 'free': {49: {555}}, 'joke': {49: {1293}}, 'here': {50: {67, 53}}, 'miles': {49: {1137}}, 'explain': {49: {453}}, 'me': {50: {25, 4}}, 'mildly': {49: {196}}, 'sits': {49: {943}}, 'sciences': {49: {505}}, 'people': {49: {389, 653, 383}}, 'bedstead': {49: {1028}}, 'to': {50: {50, 114}}, 'between': {49: {1115}}, 'five': {50: {34, 31}}, 'sex': {49: {82}}, 'why': {50: {9}}, 'linen': {49: {1015}}, 'told': {50: {48}}, 'us': {49: {1129, 987, 669}}, 'stay': {50: {57, 66, 70}}, 'like': {49: {76, 187, 657, 662, 635, 1181, 222}}, 'about': {49: {745, 684, 301, 1126}}, 'i': {50: {59, 27, 36, 5, 46}}, 'welleducated': {49: {387, 397}}, 'one': {49: {1377, 198, 233, 330, 459, 846, 1294, 340, 756}}, 'you': {50: {62, 65, 11, 22, 30}}, 'long': {49: {700}}, 'away': {50: {103}}, 'along': {50: {80}}, 'hand': {49: {563}}, 'talk': {50: {101}}, 'drink': {49: {111}}, 'scent': {49: {1016, 101}}, 'thrust': {49: {1111}}, 'sure': {49: {373}}, 'arm': {49: {908}}, 'how': {49: {689, 451, 1092}}, 'france': {49: {288, 835, 741, 296, 338, 824}}, 'germans': {49: {771}}, 'strict': {49: {1125}}, 'the': {50: {71, 39, 111, 81, 19, 89, 124, 94}}, 'tragic': {49: {904}}, 'lady': {49: {415}}, 'jumps': {49: {1229}}, 'captain': {49: {310}}, 'smoothlyshaven': {49: {30}}, 'sufferings': {49: {1465}}, 'unseemly': {49: {624}}, 'glass': {49: {242, 951}}, 'reason': {49: {1352}}, 'body': {50: {126}}, 'smeared': {49: {157}}, 'would': {49: {250, 174, 831}}, 'peace': {49: {1431}}, 'fatal': {49: {1264}}, 'likes': {49: {199}}, 'innate': {49: {498}}, 'cancan': {49: {717}}, 'feels': {49: {953}}, 'theres': {49: {464, 436}}, 'three': {49: {1134, 927}}, 'whole': {49: {184, 335}}, 'russia': {49: {1314, 366}}, 'slave': {49: {1413}}, 'give': {50: {29}}, 'flings': {49: {911}}, 'back': {49: {822}}, 'walks': {49: {920}}, 'bottles': {49: {1017}}, 'transferred': {49: {323}}, 'gambetta': {49: {854}}, 'brrr': {49: {219}}, 'dont': {49: {449, 265, 788, 1208, 634, 221}}, 'nowhere': {49: {326}}, 'mouthful': {49: {153}}, 'elegant': {49: {399}}, 'family': {49: {736}}, 'say': {49: {1296, 266, 276}}, 'keep': {49: {806}}, 'defend': {49: {754}}, 'thought': {49: {1265, 883}}, 'uproar': {49: {592}}, 'iona': {49: {627, 603}}, 'landowner': {49: {7}}, 'majestically': {49: {918}}, 'jewish': {49: {1403}}, 'ought': {49: {757, 749}}, 'into': {49: {1048, 164, 148, 834}}, 'call': {49: {1375}}, 'trembling': {49: {1007}}, 'plays': {49: {623}}, 'spirit': {49: {440, 445}}, 'well': {50: {74}}, 'none': {49: {507}}, 'begin': {49: {1210}}, 'outwardly': {49: {654}}, 'wife': {49: {706, 678}}, 'ask': {49: {1166}}, 'knows': {49: {688}}, 'ride': {49: {360}}, 'they': {49: {1123, 1155, 1222, 649, 655, 1138, 660, 830}}, 'havent': {49: {1162}}, 'he': {50: {13}}, 'now': {49: {1204, 836}}, 'feelings': {49: {897}}, 'simpleton': {50: {72, 55}}, 'up': {50: {85}}, 'fish': {49: {1344, 1362, 1286}}, 'over': {49: {1460, 900}}, 'papers': {49: {1117}}, 'bring': {49: {938}}, 'utter': {49: {1335}}, 'never': {49: {406}}, 'look': {49: {656}}, 'reluctantly': {49: {523}}, 'blue': {49: {1046}}, 'fire': {50: {112}}, 'pale': {49: {1232}}, 'grown': {49: {67}}, 'mistrustfully': {49: {1148}}, 'cassock': {50: {90}}, 'midday': {49: {5}}, 'duties': {49: {84}}, 'flaring': {49: {725}}, 'carved': {49: {606}}, 'shuts': {50: {105}}, 'play': {49: {579}}, 'goodness': {49: {687}}, 'sleepand': {49: {113}}, 'gently': {50: {109}}, 'hear': {49: {598}}, 'lord': {49: {985}}, 'falls': {50: {122}}, 'outlet': {49: {556}}, 'what': {49: {1088, 294, 967, 808, 1193, 1351, 982, 1242}}, 'ate': {49: {182}}, 'again': {49: {1461, 934}}, 'when': {49: {344, 1221, 1262, 63}}, 'enemy': {49: {879}}, 'eaten': {49: {1450}}, 'diningroom': {49: {1445}}, 'invent': {49: {568, 585}}, 'frenchman': {49: {32, 995, 583, 1065, 206, 910, 468, 405, 792}}, 'thread': {49: {621}}, 'preliminary': {49: {950}}, 'children': {49: {577, 65, 52, 1319}}, 'any': {49: {1424}}, 'kopecks': {50: {32, 35}}, 'house': {49: {709}}, 'footman': {49: {975}}, 'looks': {49: {1147}}, 'coachman': {49: {605}}, 'leaping': {49: {793}}, 'scurvy': {49: {1385}}, 'entered': {49: {1270}}, 'clutching': {49: {1252}}, 'first': {49: {1447}}, 'can': {49: {858, 331, 359, 351}}, 'in': {49: {0, 1281, 12, 783, 939, 1451, 46, 441, 316, 701, 446, 1226, 466, 339, 1113, 992, 865, 1250, 740, 874, 1002, 1011}}, 'rascal': {49: {1386}}, 'zholee': {49: {278}}, 'frenchmen': {49: {203, 636, 711}}, 'lunch': {49: {19}}, 'napkin': {49: {873, 914}}, 'home': {49: {1189}}, 'raises': {49: {1143}}, 'had': {49: {73, 41, 66, 93, 49}}, 'traitor': {49: {841}}, 'hands': {49: {1008, 876, 413}}, 'some': {49: {186, 1116, 586, 190}}, 'furnished': {49: {23}}, 'beaten': {49: {774}}, 'passport': {49: {1164, 1100, 1109}}, 'frencho': {49: {275}}, 'from': {49: {1043, 492, 1158}}, 'itself': {49: {1330}}, 'shot': {49: {163}}, 'tearstained': {49: {1437}}, 'bit': {50: {16}}, 'fork': {49: {428}}, 'death': {49: {1329}}, 'pronunciation': {49: {57}}, 'young': {50: {40, 82, 20}}, 'whatever': {49: {210}}, 'down': {49: {1239}}, 'highly': {49: {738}}, 'table': {49: {24, 947, 931, 872, 917}}, 'not': {50: {49}}, 'listener': {49: {966}}, 'break': {49: {570}}, 'fetters': {49: {1227}}, 'thing': {49: {1378, 588}}, 'everything': {49: {260, 268}}, 'on': {50: {123}}, 'passports': {49: {1127}}, 'reassured': {49: {1304}}, 'having': {49: {773}}, 'later': {50: {93}}, 'tutor': {49: {88, 45}}, 'laid': {49: {933}}, 'country': {49: {1269, 370, 844, 285}}, 'have': {49: {482, 1032, 489, 1353, 1195, 1132, 497, 146, 882, 1106, 1466, 892}}, 'been': {49: {43}}, 'however': {49: {458}}, 'off': {49: {1054}}, 'man': {50: {8, 1, 83, 21, 41}}, 'every': {49: {1332}}, 'agree': {49: {392}}, 'unable': {49: {728}}, 'performance': {49: {1458}}, 'we': {49: {1174, 767, 742, 495}}, 'acquired': {49: {491}}, 'finger': {49: {1220}}, 'braces': {49: {1019}}, 'silence': {49: {1452}}, 'detain': {49: {1085}}, 'taught': {49: {50}}, 'asks': {49: {1056, 973}}, 'ough': {49: {160}}, 'round': {49: {329, 333}}, 'smack': {49: {254}}, 'done': {49: {894}}, 'shadow': {50: {121}}, 'wideeyed': {49: {1234}}, 'men': {49: {658}}, 'queer': {49: {1361, 1090, 1285, 1343}}, 'tableall': {49: {1031}}, 'still': {49: {1060}}, 'than': {49: {384, 888, 287}}, 'correct': {49: {56, 1023}}, 'rule': {49: {639}}, 'were': {49: {89, 234, 522}}, 'must': {49: {768}}, 'forgive': {49: {986}}, 'immense': {49: {369}}, 'intelligence': {49: {490, 499, 546, 542}}, 'french': {49: {386, 802, 866, 1350, 200, 394, 172, 248, 59, 189, 510}}, 'spit': {49: {431}}, 'silent': {50: {12}}, 'bitter': {49: {1325}}, 'something': {49: {75, 477, 470}}, 'luxuriously': {49: {22}}, 'hours': {49: {928}}, 'lead': {49: {1223}}, 'month': {49: {318}}, 'abuse': {49: {1348}}, 'true': {49: {722, 403}}, 'yes': {49: {776, 364, 1151}}, 'while': {49: {704, 581, 342, 494}}, 'babbles': {49: {135}}, 'lower': {49: {1283}}, 'tone': {49: {1307, 1284}}, 'drive': {49: {346, 332}}, 'but': {50: {45}}, 'leaving': {49: {1267}}, 'room': {49: {1241, 123, 924, 1004, 15}}, 'meal': {49: {36}}, 'says': {50: {18, 38}}, 'civilization': {49: {390}}, 'sitting': {49: {697, 11, 998}}, 'outrage': {49: {890}}, 'course': {49: {1448, 549}}, 'tail': {49: {1397}}, 'writing': {49: {1214}}, 'their': {50: {97, 100}}, 'steps': {50: {98}}, 'lost': {49: {1121, 1107}}, 'sets': {49: {713}}, 'always': {49: {764}}, 'tears': {49: {144, 1040}}, 'household': {49: {48}}, 'consents': {50: {73}}, 'called': {49: {8}}, 'day': {49: {602, 699, 341}}, 'speaker': {49: {472}}, 'hate': {49: {800}}, 'another': {49: {1380, 853}}, 'prefer': {49: {191}}, 'upon': {49: {1140}}, 'somethingand': {49: {569}}, 'cant': {49: {1295}}, 'kopeck': {49: {1416}}, 'swear': {49: {1309}}, 'idle': {49: {106}}, 'thank': {49: {769}}, 'his': {50: {106, 43}}, 'your': {49: {258, 677, 582, 1159, 1318, 171, 843, 337, 849, 532, 1108, 280, 377, 509, 255}}, 'then': {50: {78}}, 'wouldnt': {49: {1179}}, 'inventive': {49: {545}}, 'wont': {49: {1217, 1082}}, 'champouns': {49: {1464}}, 'this': {49: {119, 39}}, 'mind': {49: {1203, 292}}, 'joints': {49: {170}}, 'friend': {50: {26}}, 'example': {49: {1368}}, 'be': {49: {322, 1259, 751, 372, 759, 410, 95}}, 'protests': {49: {724}}, 'scratching': {50: {42}}, 'world': {49: {596}}, 'catch': {49: {1399}}, 'them': {49: {775, 780, 719}}, 'remember': {49: {480}}, 'stick': {49: {675}}, 'random': {49: {137}}, 'a': {50: {91, 118, 15}}, 'indefinite': {49: {128}}, 'that': {49: {485, 746, 267, 1260, 784, 177, 145, 401, 115, 1171, 1176, 223}}, 'dancing': {49: {715, 61}}, 'listen': {49: {103}}, 'lazar': {49: {1370}}, 'enough': {49: {1430}}, 'ludovikovitch': {49: {970}}, 'sir': {49: {981}}, 'male': {49: {81}}, 'grow': {50: {115}}, 'greater': {49: {886}}, 'effeminacy': {49: {1038}}, 'usual': {49: {134}}, 'dinner': {49: {913, 941}}, 'dignity': {49: {926}}, 'venture': {49: {1083}}, 'received': {49: {121}}, 'is': {50: {14}}, 'should': {49: {1200, 1365}}, 'sent': {49: {312}}, 'pacing': {49: {1236}}, 'its': {49: {529, 274, 1205, 534}}, 'strange': {49: {2}}, 'under': {49: {1184}}, 'salary': {49: {129}}, 'little': {49: {304, 617, 608, 1063}}, 'standing': {49: {1059}}, 'live': {49: {661}}, 'might': {50: {37}}, 'at': {49: {418, 1253, 136, 845, 1167, 945, 20, 564, 852, 1149}}, 'concord': {49: {1433}}, 'nasty': {49: {271, 263}}, 'dead': {50: {125}}, 'antics': {49: {625}}, 'match': {49: {514}}, 'damnation': {49: {138}}, 'generally': {49: {648}}, 'do': {49: {1347, 804, 1068, 812, 176, 827}}, 'express': {49: {462}}, 'wave': {49: {905}}, 'dogs': {49: {663}}, 'time': {49: {847}}, 'who': {49: {1169, 855}}, 'doesnt': {49: {1290, 1406, 430, 423}}, 'if': {50: {51}}, 'leave': {49: {1321}}, 'rude': {49: {411}}, 'certainly': {49: {528}}, 'air': {49: {1034}}, 'ear': {49: {1392}}, 'packing': {49: {978, 1010}}, 'know': {49: {450, 743, 1104, 179, 1079, 1175}}, 'isaakitch': {49: {1371}}, 'minute': {50: {92}}, 'once': {49: {1168, 42, 670}}, 'case': {49: {785}}, 'it': {49: {161, 228, 454, 599, 521, 1112, 685, 302, 463, 558, 782, 690, 755, 374, 246, 632, 571, 252, 574, 1119}}, 'before': {49: {1130}}, 'let': {49: {832, 815}}, 'hour': {49: {1261}}, 'crushing': {49: {869}}, 'turn': {49: {328}}, 'stabs': {49: {1336}}, 'marriage': {49: {665}}, 'gets': {50: {84}}, 'could': {49: {880}}, 'complicated': {49: {91}}, 'only': {49: {1202, 547}}, 'craving': {49: {955}}, 'figure': {49: {1024}}, 'go': {49: {1072, 817, 821, 1183}}, 'married': {49: {673}}, 'or': {49: {572, 1135}}, 'boasting': {49: {565}}, 'given': {49: {553}}, 'am': {49: {641, 538, 1311, 809}}, 'jew': {49: {1382}}, 'with': {50: {24, 88, 3}}, 'where': {49: {1051}}, 'professors': {49: {511}}, 'travel': {49: {1097}}, 'better': {49: {382, 286}}, 'without': {49: {1098}}, 'among': {49: {1128}}, 'smell': {49: {99}}, 'powders': {49: {1435}}, 'himfrogs': {49: {213}}, 'same': {49: {1457, 439}}, 'does': {49: {628}}, 'dressed': {49: {97}}}\n"
     ]
    }
   ],
   "source": [
    "def readFile(file):\n",
    "#     f = open(f\"./ShortStories/{file}.txt\", encoding=\"utf8\")\n",
    "    f=open('./ShortStories/{}.txt'.format(file), encoding='utf-8')\n",
    "    \n",
    "    #   Read and Lower Case \n",
    "    data = f.read().lower()    \n",
    "        \n",
    "    #   Removing Punctuations\n",
    "    data = data.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # We have identified all unique characters and numbers and removed all others but alphabets       \n",
    "    # '9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n\n",
    "    data = data.translate(str.maketrans('','','1234567890©§“”‘’\"´¨¯—ªã'))\n",
    "        \n",
    "\n",
    "    # Word to word checking of file and adding to Dictionary\n",
    "    \n",
    "    for i, word in enumerate(data.split()):\n",
    "        if word in dictionary.keys():\n",
    "            if file in dictionary[word].keys():\n",
    "                dictionary[word][file].add(i)\n",
    "            else:\n",
    "                dictionary[word] = {file: {i}}\n",
    "        else:\n",
    "            dictionary[word] = {file: {i}}\n",
    "#             dictionary[word][file] = {i}\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "dictionary = {}\n",
    "# stopWords = readStopWords()\n",
    "# Run for Each File\n",
    "for x in range(49,51):\n",
    "    readFile(x)\n",
    "    \n",
    "# for word in stopWords:\n",
    "#     if ps.stem(word) in dictionary:\n",
    "#         print(word, \" in dictionary \")\n",
    "    \n",
    "# print(\"Dictionary Length: \", len(dictionary))\n",
    "print(dictionary)\n",
    "\n",
    "f = open(\"dictionary.txt\", \"w\")\n",
    "\n",
    "with open('dictionary.txt', 'w') as the_file:\n",
    "    for word in dictionary:\n",
    "        f.write('{}\\n'.format(word))\n",
    "    f.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding All Characters used before translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n'}\n"
     ]
    }
   ],
   "source": [
    "allchars = set()    \n",
    "for word in dictionary:\n",
    "        for letter in word:\n",
    "            allchars.add(letter)\n",
    "print(allchars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:  my long strings big  sentance antisocial my different string\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string \n",
    "\n",
    "\n",
    "    #   Read a Line and work on it and repeat\n",
    "line = \"my long string's big 9 sentance anti-social different 1str@ing!\"\n",
    "        \n",
    "    #   End of line Found\n",
    "\n",
    "line = line.lower()\n",
    "line = line.translate(str.maketrans('','',string.punctuation))\n",
    "line = line.translate(str.maketrans('','','1234567890'))\n",
    "line = line.translate(str.maketrans('','','“”‘’\"—'))\n",
    "\n",
    "# words = word_tokenize(line)\n",
    "\n",
    "print(\"Line: \", line)\n",
    "# print(\"Words: \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my  \t\t:\t my\n",
      "long  \t\t:\t long\n",
      "string  \t\t:\t string\n",
      "'s  \t\t:\t 's\n",
      "big  \t\t:\t big\n",
      "sentance  \t\t:\t sentanc\n",
      "that-make  \t\t:\t that-mak\n",
      "different  \t\t:\t differ\n",
      "1str  \t\t:\t 1str\n",
      "@  \t\t:\t @\n",
      "ing  \t\t:\t ing\n",
      "!  \t\t:\t !\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"my long string's big sentance that-make different 1str@ing!\")\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" \\t\\t:\\t\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a\n",
      "is is\n",
      "the the\n",
      "of of\n",
      "all all\n",
      "and and\n",
      "to to\n",
      "can can\n",
      "be be\n",
      "as as\n",
      "once onc\n",
      "for for\n",
      "at at\n",
      "am am\n",
      "are are\n",
      "has ha\n",
      "have have\n",
      "had had\n",
      "up up\n",
      "his hi\n",
      "her her\n",
      "in in\n",
      "on on\n",
      "no no\n",
      "we we\n",
      "do do\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "f = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = []\n",
    "while(1):\n",
    "    line = f.readline().strip()\n",
    "    if(not line):\n",
    "        break\n",
    "    stopwords.append(line)\n",
    "f.close()\n",
    "for psword in stopwords:\n",
    "    print(psword, ps.stem(psword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"stopwords.txt\", \"r\")\n",
    "# while(1):\n",
    "stopWords = f.read().split()\n",
    "for stopword in stopWords:\n",
    "    dictionary.pop(ps.stem(stopword))\n",
    "f.close()\n",
    "print(\"Dictionary Length: \", len(dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Moving to Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Length:  6789\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "ps = PorterStemmer()\n",
    "\n",
    "def readFile(file):\n",
    "#     f = open(f\"./ShortStories/{file}.txt\", encoding=\"utf8\")\n",
    "    f=open('./ShortStories/{}.txt'.format(file), encoding='utf-8')\n",
    "    \n",
    "    #   Read and Lower Case \n",
    "    line = f.read().lower()    \n",
    "        \n",
    "    #   Removing Punctuations\n",
    "    line = line.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # We have identified all unique characters and numbers and removed all others but alphabets       \n",
    "    # '9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n\n",
    "    line = line.translate(str.maketrans('','','1234567890©§“”‘’\"´¨¯—ªã'))\n",
    "        \n",
    "    for word in line.split():\n",
    "        word = ps.stem(word)\n",
    "        if word in dictionary.keys():\n",
    "            dictionary[word].add(file)\n",
    "        else:\n",
    "            dictionary[word] = {file}\n",
    "\n",
    "\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "# Run for Each File\n",
    "for x in range(1,51):\n",
    "    readFile(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Dictionary Length: \", len(dictionary))\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
