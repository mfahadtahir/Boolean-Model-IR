{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 IR | Boolean Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStopWords():\n",
    "    f = open(\"stopwords.txt\", \"r\")\n",
    "    stopWords = f.read().split()\n",
    "  \n",
    "    # making a Frozen Set as we want to use the \"Hashing\" part of it when we are processing files to ignore stopWords\n",
    "    stepWordsStem = set()\n",
    "    # We are stemming Stop words too to be able to remove all stems of a stopWord \n",
    "    # i.e if have is a stopword we want to ignore have, having, haven't and so on all stems of have \n",
    "    \n",
    "    for stopword in stopWords:\n",
    "        stepWordsStem.add(ps.stem(stopword))\n",
    "    stepWordsStem = frozenset(stepWordsStem)\n",
    "    f.close()\n",
    "    print(stopWords)\n",
    "    return stepWordsStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding Words from Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'is', 'the', 'of', 'all', 'and', 'to', 'can', 'be', 'as', 'once', 'for', 'at', 'am', 'are', 'has', 'have', 'had', 'up', 'his', 'her', 'in', 'on', 'no', 'we', 'do']\n",
      "Dictionary Length:  6763\n"
     ]
    }
   ],
   "source": [
    "def readFile(file):\n",
    "#     f = open(f\"./ShortStories/{file}.txt\", encoding=\"utf8\")\n",
    "    f=open('./ShortStories/{}.txt'.format(file), encoding='utf-8')\n",
    "    \n",
    "    #   Read and Lower Case \n",
    "    data = f.read().lower()    \n",
    "        \n",
    "    #   Removing Punctuations\n",
    "    data = data.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # We have identified all unique characters and numbers and removed all others but alphabets       \n",
    "    # '9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n\n",
    "    data = data.translate(str.maketrans('','','1234567890©§“”‘’\"´¨¯—ªã'))\n",
    "        \n",
    "\n",
    "    # Word to word checking of file and adding to Dictionary\n",
    "    \n",
    "    for word in data.split():\n",
    "        \n",
    "        word = ps.stem(word)\n",
    "        if word not in stopWords:\n",
    "#             print(word, ps.stem(word))\n",
    "\n",
    "            if word in dictionary.keys():\n",
    "                dictionary[word].add(file)\n",
    "            else:\n",
    "                dictionary[word] = {file}\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "dictionary = {}\n",
    "stopWords = readStopWords()\n",
    "# Run for Each File\n",
    "for x in range(1,51):\n",
    "    readFile(x)\n",
    "    \n",
    "# for word in stopWords:\n",
    "#     if ps.stem(word) in dictionary:\n",
    "#         print(word, \" in dictionary \")\n",
    "    \n",
    "print(\"Dictionary Length: \", len(dictionary))\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"dictionary.txt\", \"w\")\n",
    "\n",
    "with open('dictionary.txt', 'w') as the_file:\n",
    "    for word in dictionary:\n",
    "        f.write('{}\\n'.format(word))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"invertedIndex.txt\", \"w\")\n",
    "\n",
    "with open('invertedIndex.txt', 'w') as the_file:\n",
    "    for word in dictionary:\n",
    "        f.write('{}: {}\\n'.format(word, dictionary[word]))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding All Characters used before translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n'}\n"
     ]
    }
   ],
   "source": [
    "allchars = set()    \n",
    "for word in dictionary:\n",
    "        for letter in word:\n",
    "            allchars.add(letter)\n",
    "print(allchars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:  my long strings big  sentance antisocial my different string\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string \n",
    "\n",
    "\n",
    "    #   Read a Line and work on it and repeat\n",
    "line = \"my long string's big 9 sentance anti-social different 1str@ing!\"\n",
    "        \n",
    "    #   End of line Found\n",
    "\n",
    "line = line.lower()\n",
    "line = line.translate(str.maketrans('','',string.punctuation))\n",
    "line = line.translate(str.maketrans('','','1234567890'))\n",
    "line = line.translate(str.maketrans('','','“”‘’\"—'))\n",
    "\n",
    "# words = word_tokenize(line)\n",
    "\n",
    "print(\"Line: \", line)\n",
    "# print(\"Words: \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my  \t\t:\t my\n",
      "long  \t\t:\t long\n",
      "string  \t\t:\t string\n",
      "'s  \t\t:\t 's\n",
      "big  \t\t:\t big\n",
      "sentance  \t\t:\t sentanc\n",
      "that-make  \t\t:\t that-mak\n",
      "different  \t\t:\t differ\n",
      "1str  \t\t:\t 1str\n",
      "@  \t\t:\t @\n",
      "ing  \t\t:\t ing\n",
      "!  \t\t:\t !\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"my long string's big sentance that-make different 1str@ing!\")\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" \\t\\t:\\t\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a\n",
      "is is\n",
      "the the\n",
      "of of\n",
      "all all\n",
      "and and\n",
      "to to\n",
      "can can\n",
      "be be\n",
      "as as\n",
      "once onc\n",
      "for for\n",
      "at at\n",
      "am am\n",
      "are are\n",
      "has ha\n",
      "have have\n",
      "had had\n",
      "up up\n",
      "his hi\n",
      "her her\n",
      "in in\n",
      "on on\n",
      "no no\n",
      "we we\n",
      "do do\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "f = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = []\n",
    "while(1):\n",
    "    line = f.readline().strip()\n",
    "    if(not line):\n",
    "        break\n",
    "    stopwords.append(line)\n",
    "f.close()\n",
    "for psword in stopwords:\n",
    "    print(psword, ps.stem(psword))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"stopwords.txt\", \"r\")\n",
    "# while(1):\n",
    "stopWords = f.read().split()\n",
    "for stopword in stopWords:\n",
    "    dictionary.pop(ps.stem(stopword))\n",
    "f.close()\n",
    "print(\"Dictionary Length: \", len(dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Moving to Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Length:  6789\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "ps = PorterStemmer()\n",
    "\n",
    "def readFile(file):\n",
    "#     f = open(f\"./ShortStories/{file}.txt\", encoding=\"utf8\")\n",
    "    f=open('./ShortStories/{}.txt'.format(file), encoding='utf-8')\n",
    "    \n",
    "    #   Read and Lower Case \n",
    "    line = f.read().lower()    \n",
    "        \n",
    "    #   Removing Punctuations\n",
    "    line = line.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "    # We have identified all unique characters and numbers and removed all others but alphabets       \n",
    "    # '9', '0', '—', 'i', 'u', 'q', '2', 'e', 'ª', 'z', 'y', '”', '8', 'd', 'v', 'o', 'm', 'f', 'b', '¨', '7', '1', 'a', '’', 'l', 'w', '§', 'j', '“', 'x', 'g', '©', 'p', '6', '3', 'r', 'c', 't', 'ã', '5', '´', '¯', '‘', 'h', '4', 's', 'k', 'n\n",
    "    line = line.translate(str.maketrans('','','1234567890©§“”‘’\"´¨¯—ªã'))\n",
    "        \n",
    "    for word in line.split():\n",
    "        word = ps.stem(word)\n",
    "        if word in dictionary.keys():\n",
    "            dictionary[word].add(file)\n",
    "        else:\n",
    "            dictionary[word] = {file}\n",
    "\n",
    "\n",
    "    f.close()\n",
    "    return\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "# Run for Each File\n",
    "for x in range(1,51):\n",
    "    readFile(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Dictionary Length: \", len(dictionary))\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
